================================================================

    README file for Assignment 3 - Concurrent Threaded HTTP Proxy

           Name: Braeden Kepner 
		   Username/Login: bkepner
 
================================================================

I have left pertinent comments from the readme for assignment 2 in this readme.
All old information is marked with a - while new information is marked with a *. 

Bugs: No known bugs

Limitations:
	-	The proxy sends and understands only the GET method, and is not easily extendable
	to the HTTP/1.1 specification. I decided to have it parse only the first three characters
	of the method, as this made parsing simpler.
	-	The proxy does not support header folding
	-	Some of the parsing for the proxy is not as elegant as I would like. I did not know
	about the strtok function when I started the proxy, so the parsing ended up being very manual.
    * In certain places, design tradeoffs had to be made between code understandability and strict
	  performance. In order to be able to check for no-cache and private headers, the entire header
      section of the server response must be in memory. I opted to do both the recieving and the
	  checking in the proxycache_addtocache function. While this was advantageous from a processing
      and readability perspective, it also forced me to free the buffers in that function, and thus
      possibly have to malloc buffer space again later. This is necessary because realloc may change
	  the location of buf, and there's no elegant way to pass the new location and the success or failure
      of the function back to the caller. 

*Multi-threading:
	* The proxy implements support for concurrent requests using threads (pthreads library).
	* The main function serves as the dispacher (boss) thread, listening for incoming connections,
	  and then creating worker threads to service them. A maximum of MAX_NUM_THREADS (default 100)
	  can be active at one time, after which requests will wait in the listen queue (also defaulting to 100).
	* A mutex is used to protect the global variable that tracks the number of active threads.

*Caching
	* I created a structure to track what pages have been cached. The cache structure is a binary search tree, with each node 
	  value being the hashed value of one of the cached sites. I felt that this was a fair compromise between, on the one hand,
	  having a bucket for every possible SHA1 value, which would have taken too much memory, and on the other hand a simple 
      linked-list with linear search time growth. It was also reasonably simple to implement and debug in the time-period
	  available.
	* I similarly implemented a variety of static functions to deal with routine operations on the cacheTree structure.
 	* The physical cache files are stored on disk in a directory CACHE_DIRECTORY (default .proxy_cache). They are named
	  with the SHA1 hash of their URLs.
	* When the proxy recieves a request for a non-cached, but cacheable page, it first caches the page, and then sends
	  it back to the client from the cache.
	* Access to the cache is protected by a write-mutex read-semaphore structure. To write, the writing thread:
			1. acquires the write-mutex
			2. acquires all read-semaphores
			3. writes to cache
			4. releases write_mutex
			5. releases all read-semaphores
	 to read, the reading thread:
			1. acquires the write-mutex
		    2. waits() on a semaphore
			3. release write-mutex
			4. read
			5. release (increment) semaphore

	This scheme ensures that 
			(a) only one thread can be writing at a time, 
		    (b) ensures that no threads read while the cache is being written 
		    (c) allows multiple threads to read from the cache simultaneously
		    (d) by requiring posession of the write-mutex to get a read semaphore, gives some preference to writer threads
				to prevent writer starvation.
	* In practice, for the sake of modularity and readability, locks and semaphores are sometimes requested slightly before they
	  will be needed, and released slightly after they are needed. This is less efficient, but avoids the unpleasent problem of
	  passing pointers to semaphores and mutexes into helper functions.
	* The cache is not deleted upon exit or startup, but the cache tree is emptied, and so no previously cached files will be used.
 
Header Verification:
	-	The proxy performs verification on all headers to make sure that they are properly formed.
I elected to have the proxy delete any excess host headers, only sending the one parsed from
the original request. I did this for clarity, and to avoid any routing problems further down the 
line from the proxy.
	* The proxy drops malformed headers, but will send the remaining request on to the server.
	* The proxy does not verify the headers sent back from the server other than to check if they contain
 	 a no-cache directive.

Buffering:
	-	The proxy stores client requests in a buffer which grows dynamically up to a maximum size of 64kb. If the 
	client tries to send a single request of greater than 64kb, the request will be dropped, after which 
	new connections will continue to accepted.
	-	Rather than resize the buffer to take server responses, server responses are accepted up to the size
	of the buffer, at which point they are sent to the client, with recieving up to buffer size and sending
	to the client alternating until all data has been forwarded. This pattern is possible since, with the client
	request, the entire request must be in memory for parsing and verification simultaneously. When returning
	data to the client, however, no parsing or cleanup is necessary, so we can skip this step, and simply send data
	on as it is recieved. In this case, there is no need to dynamically grow the buffer.
